{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team ... (Elipsis) - BaazarVoice Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mateusz Cichy (MEng Computer Science at QUB) mcichy01@qub.ac.uk\n",
    "#### Przemek Cichy (PHD Student at UU) Cichy-P@ulster.ac.uk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We built a simple model using Stochastic Gradient Descent with binary output. We finished aroud 1pm yeaterday and were happy with the performance -  the model performs at 93% accuracy.\n",
    "\n",
    "![image.png](results.png)\n",
    "\n",
    "\n",
    "\n",
    "Since 1PM yesterday, we believed it's worth to try to build a second model - **Deep Learning Models** (**LTSM** - Long short-term memory, a type of **Recurrent Neural Network**). \n",
    "\n",
    "We were surprised to see that our second, Deep Model didn't perform as well as our first SGD mode. We believe our feature engineering was not as good hence it didn't perform as well as SGD Classifier.\n",
    "\n",
    "Today we will be presenting the SGD Classifier.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter tuning for SGD\n",
    "\n",
    "We only had 2 team members and we spent long time on Deep Learning Model (LSTM).\n",
    "In hindsight, if we wanted to achieve the best performance in this particular task, we should have spend the time on data-preprocessing and hyper-parameter tuning for the SGD. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How different parameter tuning impacts confusion matrix\n",
    "### Choosing fine balance between false positives and false negatives\n",
    "### Biasing the classifier towards never classifying real reviews as false. Do we want to flag some legit review for the sake of detecting more scam reviews or vice-versa.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n) (default=(1, 1))\n",
    "The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.\n",
    "\n",
    "## n-grams 1-2\n",
    "### better over accuracy\n",
    "![image.png](ngram12.png)\n",
    "\n",
    "\n",
    "## n-grams 1-5\n",
    "###  worse overal accuracy, however, less chance of classifying legit message as spam, i.e. less false positives\n",
    "![image.png](ngram15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF Tuning\n",
    "\n",
    "### IDF is very important, we can clearly see the difference between IDF turned on and off. It's worth to tune the parameters further. \n",
    "\n",
    "TfidfTransformer - Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.\n",
    "\n",
    "## Turned off\n",
    "![image.png](tfid-off.png)\n",
    "\n",
    "##  Turned on\n",
    "![image.png](tfid-on.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other parameters\n",
    "alpha constant that multiplies the regularization term.\n",
    "5 fold cross validation \n",
    "\n",
    "max_df = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    "max_df = 25 means \"ignore terms that appear in more than 25 documents\".\n",
    "\n",
    "max_features : int or None (default=None)\n",
    "If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "\n",
    "Because our grid search had 160 different variants its hard to plot exactly how different parameters impact the accuracy.\n",
    "\n",
    "## Data-preprocessing\n",
    "Stop-words: we tried built-in stop-word list from scikitlearn, however, it increased our testing error. We decided not to exclude any words that are at least 2 character long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
